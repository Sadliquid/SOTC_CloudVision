{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5c473b",
   "metadata": {},
   "source": [
    "# Sigils of The Codex\n",
    "\n",
    "By the end of this Code Along session, you will learn to run inferences through `Google's Cloud Vision API`, retrieve annotation labels, and sort them by recyclable categories. You will implement a **scoring system** to determine the best prediction for each image, and build a simple tool to **identify recyclable items** within images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57aa63f",
   "metadata": {},
   "source": [
    "## Install Dependancies\n",
    "We'll first need to install some dependancies. Here is a breakdown of the necessary modules:\n",
    "- `google-cloud-vision`: The official Google Cloud Vision API client library for Python.\n",
    "\n",
    "- `pillow`: A Python Imaging Library with Image Processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-vision pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79655d1",
   "metadata": {},
   "source": [
    "## Dependancy Import and Environment Setup\n",
    "We'll authenticate our Google Cloud Vision API client using the `serviceAccountKey.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tempfile\n",
    "from google.cloud import vision\n",
    "from PIL import Image\n",
    "\n",
    "# Set Credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'serviceAccountKey.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add4ff8",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "`config.json` â†’ maps categories (e.g. Plastic & Paper) to Google's Cloud Vision API labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b59b4",
   "metadata": {},
   "source": [
    "## Pre-processing (Image Optimization)\n",
    "Resize and convert all images to `JPG` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c82170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_image(image_path, max_size=(800, 800)):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img.thumbnail(max_size)\n",
    "\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n",
    "            img.save(temp_file.name, \"JPEG\", quality=85)\n",
    "\n",
    "            return temp_file.name\n",
    "    except Exception as e:\n",
    "        print(f\"Error optimizing image: {e}\")\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f10b0",
   "metadata": {},
   "source": [
    "## Object Localization\n",
    "`ImageAnnotatorClient` performs **Object Localization** on the images. We'll construct a collection of annotation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0386784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_objects(image_path):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    try:\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        response = client.object_localization(image=image)\n",
    "        objects = response.localized_object_annotations\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f'Error: {response.error.message}')\n",
    "\n",
    "        detected_objects = [obj.name for obj in objects]\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf57cf4",
   "metadata": {},
   "source": [
    "## Object Mapping\n",
    "We'll map the detected objects to their corresponding categories to find the most relevant category for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75662f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recyclable_categories(detected_objects):\n",
    "    matching_categories = set()\n",
    "    for category, terms in config.items():\n",
    "        if any(obj in terms for obj in detected_objects):\n",
    "            matching_categories.add(category)\n",
    "    return list(matching_categories) if matching_categories else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ca8d7",
   "metadata": {},
   "source": [
    "## Handling Edge-Case (Multiple Objects Detected)\n",
    "In the likely scenario that multiple objects are detected in an image, we will implement a **scoring system** to determine the best-fit category based on the **object overlaps**. In the scenario that there is still a tie between multiple categories, we will perform `Label Detection` on the image to get a more granular analysis of the detected objects. This will help us identify the most relevant category based on the most occurences of label matches across a wider range of generic labels.\n",
    "\n",
    "`ImageAnnotatorClient` performs **Label Detection** on the images. We'll construct a collection of annotation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def granular_analysis_to_resolve_tie(file_name, image_path, tied_categories):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    try:\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = [label.description for label in response.label_annotations]\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f'Error: {response.error.message}')\n",
    "\n",
    "        label_match_count = {category: 0 for category in tied_categories}\n",
    "        for category in tied_categories:\n",
    "            terms = config.get(category, [])\n",
    "            label_match_count[category] = sum(1 for label in labels if label in terms)\n",
    "\n",
    "        best_category = max(label_match_count, key=label_match_count.get)\n",
    "        return best_category if label_match_count[best_category] > 0 else None\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def get_best_fitting_category(file_name, image_path, detected_objects, matched_categories):\n",
    "    category_match_count = {category: 0 for category in matched_categories}\n",
    "    for category in matched_categories:\n",
    "        terms = config.get(category, [])\n",
    "        category_match_count[category] = sum(1 for obj in detected_objects if obj in terms)\n",
    "\n",
    "    best_category = max(category_match_count, key=category_match_count.get)\n",
    "    highest_count = category_match_count[best_category]\n",
    "\n",
    "    tied_categories = [cat for cat, count in category_match_count.items() if count == highest_count]\n",
    "\n",
    "    if len(tied_categories) > 1:\n",
    "        return granular_analysis_to_resolve_tie(file_name, image_path, tied_categories)\n",
    "\n",
    "    return best_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527f29c",
   "metadata": {},
   "source": [
    "## Putting it to the test!\n",
    "\n",
    "Upload an image of a **paper bag** and run an inference through our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973828eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_path):\n",
    "    optimized_path = optimize_image(image_path)\n",
    "\n",
    "    detected_objects = get_detected_objects(optimized_path)\n",
    "    if isinstance(detected_objects, dict) and 'error' in detected_objects:\n",
    "        print(f\"Error detecting objects: {detected_objects['error']}\")\n",
    "        return\n",
    "\n",
    "    if not detected_objects:\n",
    "        print(\"Category: No match\")\n",
    "        return\n",
    "\n",
    "    recyclable_categories = get_recyclable_categories(detected_objects)\n",
    "    if recyclable_categories:\n",
    "        best_category = get_best_fitting_category(\n",
    "            file_name=os.path.basename(image_path),\n",
    "            image_path=image_path,\n",
    "            detected_objects=detected_objects,\n",
    "            matched_categories=recyclable_categories\n",
    "        )\n",
    "        if best_category:\n",
    "            print(f\"{best_category} is recyclable!\")\n",
    "            os.remove(optimized_path)\n",
    "            return\n",
    "\n",
    "    print(\"Category: No match\")\n",
    "    os.remove(optimized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecde84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"images/giftBag.jpg\"\n",
    "analyze_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c4449",
   "metadata": {},
   "source": [
    "## Understanding Annotation Labels\n",
    "Try uploading an image of a **cardboard box** and run an inference through the pipeline. Do we still get a valid prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"images/cardboardBox.jpg\"\n",
    "analyze_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec46e50",
   "metadata": {},
   "source": [
    "Our pipeline does not recognise the **cardboard box**, as we do not have a collection of recognised annotation labels for `Cardboard`. The Google Cloud Vision API has a large global collection of annotation labels, and our pipeline could not find any of them in our `config.json` file.\n",
    "\n",
    "Let's fix that!\n",
    "\n",
    "First, let's build a **helper function** that fetches labels for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab78992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_labels(image_path):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = [label.description for label in response.label_annotations]\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f'Error: {response.error.message}')\n",
    "\n",
    "        return labels\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706dd7b",
   "metadata": {},
   "source": [
    "Now, let's build a **helper function** that automates the population of our `config.json` file with the labels returned by Google's Cloud Vision API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_config(category_type, image_file):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    try:\n",
    "        with open(image_file, \"rb\") as image_file_obj:\n",
    "            content = image_file_obj.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = [label.description for label in response.label_annotations]\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f'Error: {response.error.message}')\n",
    "\n",
    "        if os.path.exists(\"config.json\"):\n",
    "            with open(\"config.json\", \"r\") as f:\n",
    "                config = json.load(f)\n",
    "        else:\n",
    "            config = {}\n",
    "\n",
    "        if category_type not in config:\n",
    "            config[category_type] = []\n",
    "\n",
    "        existing_labels = set(config[category_type])\n",
    "        new_labels = [label for label in labels if label not in existing_labels]\n",
    "\n",
    "        if new_labels:\n",
    "            config[category_type].extend(new_labels)\n",
    "\n",
    "            with open(\"config.json\", \"w\") as f:\n",
    "                json.dump(config, f, indent=4)\n",
    "\n",
    "        return {\n",
    "            \"category\": category_type,\n",
    "            \"new_labels_added\": len(new_labels),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39d3f0",
   "metadata": {},
   "source": [
    "Now, let's run the image of the **cardboard box** through our `populate_config` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"images/cardboardBox.jpg\"\n",
    "category_type = \"Cardboard\"\n",
    "\n",
    "print(populate_config(category_type, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5c346",
   "metadata": {},
   "source": [
    "Great! Now, the **cardboard box** is recognised as a recyclable item, and we have added the new labels to our `config.json` file. Let's run more new images through our pipeline to expand our array of recognised labels.\n",
    "\n",
    "Why? This **improves reliability and accuracy** of our pipeline, ensuring it can handle a wider variety of recyclable items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\"images/cardboardBoxTrain_1.jpg\", \"images/cardboardBoxTrain_2.jpg\", \"images/cardboardBoxTrain_3.jpg\"]\n",
    "\n",
    "for image in images:\n",
    "    new_labels = print(populate_config(category_type, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683fb47",
   "metadata": {},
   "source": [
    "Now, let's test our improved pipeline with a **new** image of a **cardboard box**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"images/newCardboardBox.jpg\"\n",
    "analyze_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4c902",
   "metadata": {},
   "source": [
    "Great! Now we have a collection of recognised labels for `Cardboard` from Google's Cloud Vision API in our `config.json` file. Remember, the more annotation labels we have in our collection, the more **reliable** our system will be at identifying recyclable items. This could require hundreds of inferences!\n",
    "\n",
    "Let's take a look at the annotation labels we have retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941edf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config[\"Cardboard\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
