{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5c473b",
   "metadata": {},
   "source": [
    "# Sigils of The Codex\n",
    "\n",
    "By the end of this Code Along session, you will learn to run inferences through **Google's Cloud Vision API**, retrieve the returned labels, and sort them by recyclabile categories. You will implement a **scoring system** to determine the prediction with the highest confidence rate for each image, and build a simple tool to **identify recyclable items** within images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57aa63f",
   "metadata": {},
   "source": [
    "## Install Dependancies\n",
    "We'll first need to install some dependancies. Here is a breakdown of the necessary modules:\n",
    "- `google-cloud-vision`: The official Google Cloud Vision API client library for Python.\n",
    "\n",
    "- `pillow`: A Python Imaging Library with Image Processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-vision pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79655d1",
   "metadata": {},
   "source": [
    "## Dependancy Import and Environment Setup\n",
    "We'll authenticate our Google Cloud Vision API client using the `serviceAccountKey.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tempfile\n",
    "from google.cloud import vision\n",
    "from PIL import Image\n",
    "\n",
    "# Set Credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'serviceAccountKey.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add4ff8",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "`category_map.json` â†’ maps categories (e.g., plastic, paper) to Google Cloud's Vision API labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('category_map.json', 'r') as f:\n",
    "    category_map = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b59b4",
   "metadata": {},
   "source": [
    "## Pre-processing (Image Optimization)\n",
    "Resize and convert all images to JPEG format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c82170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_image(image_path, max_size=(800, 800)):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img.thumbnail(max_size)\n",
    "\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n",
    "            img.save(temp_file.name, \"JPEG\", quality=85)\n",
    "\n",
    "            return temp_file.name\n",
    "    except Exception as e:\n",
    "        print(f\"Error optimizing image: {e}\")\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f10b0",
   "metadata": {},
   "source": [
    "## Object Localization\n",
    "`ImageAnnotatorClient` performs Object Localization on the images. We'll construct a collection of annotations associated with each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0386784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_objects(image_path):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    try:\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        response = client.object_localization(image=image)\n",
    "        objects = response.localized_object_annotations\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f'Error: {response.error.message}')\n",
    "\n",
    "        detected_objects = [obj.name for obj in objects]\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf57cf4",
   "metadata": {},
   "source": [
    "## Object Mapping\n",
    "We'll map the detected objects to their corresponding categories to find the most relevant category for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75662f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recyclable_categories(detected_objects):\n",
    "    matching_categories = set()\n",
    "    for category, terms in category_map.items():\n",
    "        if any(obj in terms for obj in detected_objects):\n",
    "            matching_categories.add(category)\n",
    "    return list(matching_categories) if matching_categories else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ca8d7",
   "metadata": {},
   "source": [
    "## Handling Edge-Case (Multiple Objects Detected)\n",
    "In the likely scenario that multiple objects are detected in an image, we will implement a scoring system to determine the best-fit category based on the object overlaps. In the scenario that there is still a tie between multiple categories, we will perform **Label Detection** on the image to get a more granular analysis of the detected objects. This will help us identify the most relevant category based on the highest confidence score across a wider range of generic labels.\n",
    "\n",
    "`ImageAnnotatorClient` performs Label Detection on the images. We'll construct a collection of annotations associated with each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def granular_analysis_to_resolve_tie(file_name, image_path, tied_categories):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    try:\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = [label.description for label in response.label_annotations]\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f'Error: {response.error.message}')\n",
    "\n",
    "        label_match_count = {category: 0 for category in tied_categories}\n",
    "        for category in tied_categories:\n",
    "            terms = category_map.get(category, [])\n",
    "            label_match_count[category] = sum(1 for label in labels if label in terms)\n",
    "\n",
    "        best_category = max(label_match_count, key=label_match_count.get)\n",
    "        return best_category if label_match_count[best_category] > 0 else None\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def get_best_fitting_category(file_name, image_path, detected_objects, matched_categories):\n",
    "    category_match_count = {category: 0 for category in matched_categories}\n",
    "    for category in matched_categories:\n",
    "        terms = category_map.get(category, [])\n",
    "        category_match_count[category] = sum(1 for obj in detected_objects if obj in terms)\n",
    "\n",
    "    best_category = max(category_match_count, key=category_match_count.get)\n",
    "    highest_count = category_match_count[best_category]\n",
    "\n",
    "    tied_categories = [cat for cat, count in category_match_count.items() if count == highest_count]\n",
    "\n",
    "    if len(tied_categories) > 1:\n",
    "        return granular_analysis_to_resolve_tie(file_name, image_path, tied_categories)\n",
    "\n",
    "    return best_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527f29c",
   "metadata": {},
   "source": [
    "## Putting it to the test!\n",
    "\n",
    "Upload an image and run an inference through our pipeline to retrieve detected objects and categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973828eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_path):\n",
    "    optimized_path = optimize_image(image_path)\n",
    "\n",
    "    detected_objects = get_detected_objects(optimized_path)\n",
    "    if isinstance(detected_objects, dict) and 'error' in detected_objects:\n",
    "        print(f\"Error detecting objects: {detected_objects['error']}\")\n",
    "        return\n",
    "\n",
    "    if not detected_objects:\n",
    "        print(\"Category: No match\")\n",
    "        return\n",
    "\n",
    "    recyclable_categories = get_recyclable_categories(detected_objects)\n",
    "    if recyclable_categories:\n",
    "        best_category = get_best_fitting_category(\n",
    "            file_name=os.path.basename(image_path),\n",
    "            image_path=image_path,\n",
    "            detected_objects=detected_objects,\n",
    "            matched_categories=recyclable_categories\n",
    "        )\n",
    "        if best_category:\n",
    "            print(f\"{best_category} is recyclable!\")\n",
    "            os.remove(optimized_path)\n",
    "            return\n",
    "\n",
    "    print(\"Category: No match\")\n",
    "    os.remove(optimized_path)\n",
    "\n",
    "test_image = \"images/giftBag.jpg\"\n",
    "analyze_image(test_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
